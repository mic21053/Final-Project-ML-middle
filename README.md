# Final-Project-ML-middle
# Распознавание эмоций на фото, видео и с web-камеры

<p align="center"><img src="/imgs/Эмоции.webp" width="500" alt="Эмоции"></p>

## Пошаговая инструкция по установке и запуску модели распознавания эмоций
  * [Шаг 1. Создание виртуальной среды](#создание_виртуальной_среды)
  * [Шаг 2. Загрузка зависимостей](#загрузка_зависимостей)
  * [Шаг 3. Тест различных моделей и выбор наилучшей](#тест_различных_моделей_и_выбор_наилучшей)
## Создание виртуальной среды
Сразу оговоримся, что модель создавалась в операционной системе UBUNTU 20.04.6 LTS. Для другой операционной системы потребуются корректировки каких то команд.\
Итак, создадим папку под проект. Назовите ее как вам будет удобно. Клонируйте этот репозиторий в созданную папку: https://github.com/mic21053/Final-Project-ML-middle  
Обратите внимание - так как github не дает загружать большие по объему файлы, в папках train, test_kaggle, My_model и MyVA_model находится не содержимое этих папок, а текстовые файлы со ссылками, по которым можно скачать соответствующий контент и поместить его в эти папки.  
Далее создаем виртуальную среду:
<pre>
python -m venv <название вашей виртуальной среды>
</pre>
Активируем её:
<pre>
source <название вашей виртуальной среды>/bin/activate
</pre>
Создаем зависимости и добавляем виртуальную среду в ядро jupyter notebook:
<pre>
python -m pip install --upgrade pip
pip install ipykernel
python -m ipykernel install --user --name=<название вашей виртуальной среды>
</pre>
Запускаем jupyter notebook и выбираем в качестве ядра наше созданное виртуальное окружение. Например на скриншоте ниже - это ядро tfod.
<p align="center"><img src="/imgs/Jupyter_view.png" width="500" alt="Эмоции"></p>
В дальнейшем не забываем заходить в виртуальную среду и выходить из неё командой:
<pre>
deactivate
</pre>

## Загрузка зависимостей
В репозитории присутствует файл requirements.txt. С его помощью загружаем все необходимые библиотеки и зависимости, которые я использовал.
<pre>
pip install -r requirements.txt
</pre>

## Тест различных моделей и выбор наилучшей
Для выбора модели распознавания эмоций я воспользовался таблицей из документации keras, в которой представлены различные модели, обученные на датасете imagenet. В данной таблице представленв точность этих моделей, их размер и время инференса. Ссылка на документацию: https://keras.io/api/applications/  
Тесты всех моделей приведены в файле Neural network test.ipynb  
Все выбранные мной предобученные модели сначала замораживались и мной дописывалась только "верхушка" сети (transfer learning), а затем размораживалось несколько последних слоев и сеть дообучалась на нашем train наборе данных.  
Эти обучения сетей заняли огромное количество времени, так как архитектуры достаточно сложные и вычислений очень много.  
После каждого обучения формировался файл ответа для теста на private kaggle, который отсылался на kaggle и получал соответствующую оценку. Также проверялось время инференса на google colab, которое не должно было превышать 1/25 секунды, так как сеть должна была распознавать эмоции в том числе на видео в режиме реального времени с вебкамеры, соответственно успевая это сделать за один кадр. Так как частота кадров в стандартном видео 25 fps - вот мы и получаем время одного кадра 1/25 секунды.  
Все результаты обучения и инференса моделей сведены в итоговую таблицу в конце файла Neural network test.ipynb, на основании которой и выбрана наилучшая по оценке на kaggle сеть - ConvNeXtXLarge. Лучший чекпойнт обучения данной сети помещен в папку My_model.


