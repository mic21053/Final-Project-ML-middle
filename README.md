# Final-Project-ML-middle
# Распознавание эмоций на фото, видео и с web-камеры

<p align="center"><img src="/imgs/Эмоции.webp" width="500" alt="Эмоции"></p>

## Пошаговая инструкция по установке и запуску модели распознавания эмоций
  * [Шаг 1. Создание виртуальной среды](#создание_виртуальной_среды)
  * [Шаг 2. Загрузка зависимостей](#загрузка_зависимостей)
  * [Шаг 3. Тест различных моделей и выбор наилучшей](#тест_различных_моделей_и_выбор_наилучшей)
  * [Шаг 4. Эксперименты с исходными данными](#эксперименты_с_исходными_данными)
  * [Шаг 5. Valence-Arousal подход](#valence_arousal_подход)
  * [Шаг 6. Создание классов FaceEmotionsModel() и FrameWithEmotion()](#создание_классов_faceemotionsmodel()_и_framewithemotion())
  * [Шаг 7. Определение эмоции на фото](#определение_эмоции_на_фото)
  * [Шаг 8. Определение эмоции на видео](#определение_эмоции_на_видео)
  * [Шаг 9. Определение эмоции с web-камеры](#определение_эмоции_с_web_камеры)
## Создание виртуальной среды
Сразу оговоримся, что модель создавалась в операционной системе UBUNTU 20.04.6 LTS. Для другой операционной системы потребуются корректировки каких то команд.\
Итак, создадим папку под проект. Назовите ее как вам будет удобно. Клонируйте этот репозиторий в созданную папку: https://github.com/mic21053/Final-Project-ML-middle  
Обратите внимание - так как github не дает загружать большие по объему файлы, в папках train, test_kaggle, My_model и MyVA_model находится не содержимое этих папок, а текстовые файлы со ссылками, по которым можно скачать соответствующий контент и поместить его в эти папки.  
Далее создаем виртуальную среду:
<pre>
python -m venv <название вашей виртуальной среды>
</pre>
Активируем её:
<pre>
source <название вашей виртуальной среды>/bin/activate
</pre>
Создаем зависимости и добавляем виртуальную среду в ядро jupyter notebook:
<pre>
python -m pip install --upgrade pip
pip install ipykernel
python -m ipykernel install --user --name=<название вашей виртуальной среды>
</pre>
Запускаем jupyter notebook и выбираем в качестве ядра наше созданное виртуальное окружение. Например на скриншоте ниже - это ядро tfod.

<p align="center"><img src="/imgs/Jupyter_view.png" width="500" alt="Ядро"></p>

В дальнейшем не забываем заходить в виртуальную среду и выходить из неё командой:
<pre>
deactivate
</pre>

## Загрузка зависимостей
В репозитории присутствует файл requirements.txt. С его помощью загружаем все необходимые библиотеки и зависимости, которые я использовал.
<pre>
pip install -r requirements.txt
</pre>

## Тест различных моделей и выбор наилучшей
Для выбора модели распознавания эмоций я воспользовался таблицей из документации keras, в которой представлены различные модели, обученные на датасете imagenet. В данной таблице представленв точность этих моделей, их размер и время инференса. Ссылка на документацию: https://keras.io/api/applications/  
Тесты всех моделей приведены в файле Neural network test.ipynb  
Все выбранные мной предобученные модели сначала замораживались и мной дописывалась только "верхушка" сети (transfer learning), а затем размораживалось несколько последних слоев и сеть дообучалась на нашем train наборе данных.  
Эти обучения сетей заняли огромное количество времени, так как архитектуры достаточно сложные и вычислений очень много.  
После каждого обучения формировался файл ответа для теста на private kaggle, который отсылался на kaggle и получал соответствующую оценку. Также проверялось время инференса на google colab, которое не должно было превышать 1/25 секунды, так как сеть должна была распознавать эмоции в том числе на видео в режиме реального времени с вебкамеры, соответственно успевая это сделать за один кадр. Так как частота кадров в стандартном видео 25 fps - вот мы и получаем время одного кадра 1/25 секунды.  
Все результаты обучения и инференса моделей сведены в итоговую таблицу в конце файла Neural network test.ipynb, на основании которой и выбрана наилучшая по оценке на kaggle сеть - ConvNeXtXLarge. Лучший чекпойнт обучения данной сети помещен в папку My_model.

## Эксперименты с исходными данными
Первый эксперимент подробно описан и представлен в файле Neural net without uncertain.ipynb. Кратко его суть была в том, чтобы исключить эмоции из папки исходных данных uncertain из обучения, так как эта папка содержит совершенно различные эмоции, не подходящие под восемь основных эмоций, представленных нам для обучения. Поэтому данная мешанина не связанных между собой эмоций должна была сбивать сеть и приводить к ухудшению качества прогнозирования. Далее при предсказании эмоции выбирались различные минимальные пороги вероятности ее определения, при которых мы считали, что вероятность настолько мала, что данную эмоцию нужно отнести к неопределенной и поместить ее в папку uncertain. Улучшить результат на kaggle этот эксперимент не смог.  
Второй эксперимент - в файле Ordered neural network.ipynb. Его суть в том, что мной было замечено, что на самом деле эмоции распределены по соответствующим папкам крайне хаотично. Например, одна и та же фотография страшной рожи Джека Николсона из фильма "Сияние" есть почти в каждой папке с эмоциями. Вообще много несоответствий реальных эмоций их папкам. Вручную перебирать все было очень долго, поэтому была взята лучшая сеть из первоначального теста моделей и с помощью нее были распределены эмоции по папкам уже с точки зрения обученной сети. Далее вручную эти папки были проверены и несоответствия перемещались в нужные папки. Так я видел, где сеть ошибается больше всего.  
Было перетасовано порядка 10 тыс фото. Этого уже было достаточно для того, чтобы при переобучении сети на новом перетасованном датасете понять, приведет ли это к улучшению результата.  
Итог интересен. Оба этих эксперимента привели к ухудшению результата на kaggle. Но при этом качество перетасованной сети само по себе улучшилось. Это говорит о том, что на kaggle данные так же "зашумлены", как и данные нам для обучения, поэтому улучшение исходных данных, позволяющее реально улучшить качество прогноза сети, не приводит к улучшению оценки на kaggle.

## Valence-Arousal подход
Это подход, основанный на том, что у каждой эмоции есть свои "координаты", где по оси х откладывается "знак" эмоции (положительность или отрицательность), а по оси y - "интенсивность" эмоции.  
Для вычисления "координат" данных нам эмоций была взята наиболее распространенная в интернете схема.

<p align="center"><img src="/imgs/valence-arousal.jpg" width="500" alt="Схема"></p>

Подобно все описано в файле Valence-Arousal.ipynb. Упомяну только то, что датасета для обучения данной модели получить бесплатно нереально. Поэтому пришлось искусственно его создать на базе данного. Поэтому точность модели крайне низкая, так как для генрации "координат" эмоций использовалась наша лучшая модель, точность которой сама по себе порядка 45%. Да плюс добавляется элемент рандомности. Но нам и не нужна была точность в этом случае. Главное было - само построение такой сети. Если достать настоящий обучающий датасет, то на созданной мной архитектуре можно получить сеть с хорошими результатами. Лучший чекпойнт обученной модели находится в папке MyVA_model.

## Создание классов FaceEmotionsModel() и FrameWithEmotion()
В файлах mymodel.py и frame_drawing.py прописаны скрипты для создания классов:  
FaceEmotionsModel() - класс, загружающий соответственно переданному параметру va либо нашу выбранную по итогам тестов лучшую сеть, либо сеть, основанную на valence-arousal подходе  
FrameWithEmotion() - класс, находящий на изображении лицо и рисующий вокруг него рамку с подписанной эмоцией, ее вероятностью и в случае valence-arousal сети координатами.

## Определение эмоции на фото
Для теста скриптов используется файл Notebook for testing.ipynb. Для определения эмоции по фото запускаем скрипт emo_on_photo.py. В качестве параметра передаем путь до фото либо в случае valence-arousal подхода передаем вторым параметром va.  
Пример вызова и результат для основной сети:
<pre>
!python3 emo_on_photo.py ./test_kaggle/29.jpg
</pre>

<p align="center"><img src="/imgs/Девочка_злость.png" width="500" alt="Злая_девочка"></p>

<pre>
!python3 emo_on_photo.py ./test_kaggle/43.jpg va
</pre>

<p align="center"><img src="/imgs/Девушка_отвращение.png" width="500" alt="Злая_девушка"></p>

Надо сказать, что библиотека для захвата видео OpenCV плохо "дружит" с Ubuntu. А точнее с командой cv2.waitKey(). Пришлось найти решение, не убивающее ядро при использовании этой клавиши. Поэтому для выхода из показа используется нажатие любой клавиши на клавиатуре.  
Также не всегда хорошо справляется с определением лица на изображении алгоритм haarcascades. В соответствующей папке находятся файлы с четыремя вариантами этого алгоритма. Можете поэкспериментировать и в соответствующих строках скриптов по определению эмоций, а именно trained_face_data = cv2.CascadeClassifier('./haarcascades/haarcascade_frontalface_alt2.xml'), поменять название файла xml на один из четырех вариантов.

## Определение эмоции на видео
Для определения эмоции по видео запускаем скрипт emo_on_video.py. В качестве параметра передаем путь до видео либо в случае valence-arousal подхода передаем вторым параметром va.  
Пример вызова и результат для основной сети:
<pre>
!python3 emo_on_video.py Эмоции.avi
</pre>

<p align="center"><img src="/imgs/Счастлив.png" width="500" alt="Счастлив"></p>

<pre>
!python3 emo_on_video.py Эмоции.avi va
</pre>

<p align="center"><img src="/imgs/Счастлив2.png" width="500" alt="Счастлив2"></p>

## Определение эмоции с web-камеры
Для определения эмоции с web-камеры запускаем скрипт emo_on_web.py. В качестве параметра в случае valence-arousal подхода передаем va.  
Пример вызова и результат для основной сети:
